| [Home](index.md) | [Lectures](lectures.md) | [Labs](labs.md) | [Assignments](assignments.md) | [Project](project.md)| [Contact](contact.md) |


### Lecture Slides

ISLR = Introduction to Statistical Learning
PDS = Python for Data Science
(see homepage for links)

1. Week 1
- September 3
  * [Intro to Data Science](lectures/Intro%20to%20data%20science.pdf)
  * [Data Transformation](lectures/data_transformations.pdf)
- September 5, 
  * [Tidy Data](lectures/tidy.pdf)
  * [Data Visualization](lectures/vis.pdf)
  * [Tidy Intro](lectures/pew.html), [Tidy Advanced](lectures/billboard.html), and
  [Vis](lectures/gapminder.html) notebooks

2. Week 2
- September 10
  * [Function Fitting Intro](https://observablehq.com/@krisrs1128/function-fitting)
  * [code examples](lectures/model_families_gallery.html)
  * Reading: ISLR 2.1, 3.2.1, 3.5
- September 12
  * [Function Fitting Algorithms](https://observablehq.com/@krisrs1128/function-fitting-crash-course)
  * [demo](https://observablehq.com/@krisrs1128/knn-bias-variance)
  * Reading: ISLR 3.2.1, 3.3.1, 3.3.2, 3.5, 4.3, 7.2, 8.1

3. Week 3
- September 17
  * Function Fitting Implementation
  * Reading: PDS pg. 390 - 396, 400 - 405 and ISLR 4.6.6
  * Optional reading: ISLR 8.3.1, 8.3.2
  * Notebooks: [Regression](https://colab.research.google.com/drive/1Ro8Jp975pBuW5DdljGmqXfMqSESFzfdY), [knn + logistic regression](https://colab.research.google.com/drive/1ZyUp1v7oaN8z0qk4Y-F_Dxz1TkBRFlNh), and [trees](https://colab.research.google.com/drive/1tv6npC_FnojKAo89zAHBWQColjYKQ-rd)
- September 19
  * [Unsupervised Learning Algorithms](https://observablehq.com/@krisrs1128/unsupervised-learning)
  * Reading: ISLR 10.1, 10.2, PDS pg. 462 - 476
  * Optional Reading: ISLR 10.3

4. Week 4
- September 24
  * [Cross Validation + Model Selection](https://observablehq.com/@krisrs1128/cross-validation-and-model-selection)
  * Reading: ISLR 5.1, PDS pg. 359 - 375
- September 26
  * Introduction to Inference
  * Reading: ISLR 3.1.1, 3.1.2, [Bayesian Basics](https://m-clark.github.io/bayesian-basics/)
  * Optional reading: [MSMB](http://web.stanford.edu/class/bios221/book/Chap-Testing.html) 6.1 - 6.6, [Statistics for Hackers](https://speakerdeck.com/jakevdp/statistics-for-hackers?slide=138)

5. Week 5
- October 1
  * The Bootstrap
  * Reading: ISLR 5.2 - 5.3
  * Optional reading: [CASI](https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf) Chapter 1
- October 3
  * Large Scale Inference and Experimental Design
  * Reading: [MSMB](http://web.stanford.edu/class/bios221/book/Chap-Testing.html) 6.7 - 6.11, 13.1 - 13.4. 
  * Optional reading: [CASI](https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf) 15.1 - 15.3

6. Week 6
- October 8
  * Feature engineering + Outlier and error analysis
  *
- October 10
  * Feature selection + Dimentionallity reduction
  *
  
7. Week 7
- October 15
  * Data Bias: Source and types of data bias
  *
- October 17
  * Bias and Discrimination in Machine Learning: fairness metrics 
  *
  
8. Week 8 
- October 22 
  * Reading week
  
9. Week 9
- October 29
  * **MIDTERM**

10. Week 10
- November 5
  * Project presentation
  *
- November 7
  * Text mining (NLP): BOW, n-grams, topic model, and language model
  *
  
11. Week 11
- November 12
  *  Time series and GEO location
  *
- November 14
  * Computer Vision and Multimodal
  *
  
12. Week 12
- November 19
  * Graph mining: Information Retrieval
  *
- November 21
  * Graph mining: Recommender system
  *
  
13. Week 13
- November 26
  * Advanced Inference
  *
- November 28
  * Ensemble methods: bagging/boosting
  *
  
14. Week 14
- December 3
  * Data at scale
  *
- December 5
  * Privacy + Transparency+ Explainable data science

15. Week 15
- December 10
  * **FINAL**

